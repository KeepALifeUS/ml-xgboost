"""
Advanced Feature Engineering for XGBoost Tree Models
====================================================

Comprehensive feature engineering pipeline specifically optimized for tree-based models
and time series data. Implements enterprise patterns for
production-ready feature generation.

Key Features:
- Tree-optimized feature transformations
- Time Series-specific indicators
- Time series feature engineering
- Automated feature selection
- Feature interaction detection
- Performance optimization with caching

Author: ML XGBoost Contributors
Version: 1.0.0
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any, Union, Callable, Set
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from enum import Enum
import pickle
import joblib
import json
from pathlib import Path
import time
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import (
    StandardScaler, RobustScaler, MinMaxScaler, QuantileTransformer,
    KBinsDiscretizer, LabelEncoder, OneHotEncoder
)
from sklearn.feature_selection import (
    SelectKBest, SelectPercentile, f_regression, f_classif,
    mutual_info_regression, mutual_info_classif,
    VarianceThreshold, SelectFromModel
)
from sklearn.decomposition import PCA, FastICA
from sklearn.impute import SimpleImputer, KNNImputer
import pandas_ta as ta
import talib
import scipy.stats as stats
from scipy.signal import find_peaks
from loguru import logger
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from rich.console import Console
from rich.table import Table
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import numba
from numba import jit, prange


class FeatureType(Enum):
    """–¢–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–¥ tree –º–æ–¥–µ–ª–∏"""
    NUMERICAL = "numerical"
    CATEGORICAL = "categorical"
    BINARY = "binary"
    ORDINAL = "ordinal"
    TEMPORAL = "temporal"
    INTERACTION = "interaction"
    RATIO = "ratio"
    DIFFERENCE = "difference"
    LOG_TRANSFORM = "log_transform"
    BINNED = "binned"


class ScalingMethod(Enum):
    """–ú–µ—Ç–æ–¥—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    NONE = "none"
    STANDARD = "standard"  # Z-score normalization
    ROBUST = "robust"  # Median and IQR
    MINMAX = "minmax"  # Min-max scaling
    QUANTILE = "quantile"  # Quantile transformation
    LOG = "log"  # Log transformation
    SQRT = "sqrt"  # Square root transformation


@dataclass
class FeatureConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è feature engineering"""
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    enable_technical_indicators: bool = True
    enable_statistical_features: bool = True
    enable_lag_features: bool = True
    enable_interaction_features: bool = True
    enable_volatility_features: bool = True
    
    # Technical indicators
    technical_periods: List[int] = field(default_factory=lambda: [5, 10, 14, 20, 30, 50])
    enable_oscillators: bool = True
    enable_trend_indicators: bool = True
    enable_volume_indicators: bool = True
    
    # Statistical features
    statistical_windows: List[int] = field(default_factory=lambda: [5, 10, 20, 50])
    enable_moments: bool = True  # skewness, kurtosis
    enable_quantiles: bool = True
    enable_entropy: bool = True
    
    # Lag features
    max_lags: int = 20
    lag_windows: List[int] = field(default_factory=lambda: [1, 2, 3, 5, 10, 15, 20])
    enable_rolling_lags: bool = True
    
    # Interaction features
    max_interaction_degree: int = 2
    interaction_threshold: float = 0.1  # Minimum correlation for interaction
    auto_detect_interactions: bool = True
    
    # Volatility features  
    volatility_windows: List[int] = field(default_factory=lambda: [5, 10, 20, 30])
    enable_garch_features: bool = True
    enable_realized_volatility: bool = True
    
    # Feature selection
    enable_feature_selection: bool = True
    feature_selection_method: str = "mutual_info"  # mutual_info, f_score, variance
    feature_selection_k: int = 100  # Top K features
    variance_threshold: float = 0.01
    
    # Data preprocessing
    handle_missing: str = "forward_fill"  # forward_fill, interpolate, knn, drop
    scaling_method: ScalingMethod = ScalingMethod.ROBUST
    remove_outliers: bool = True
    outlier_method: str = "iqr"  # iqr, zscore, isolation_forest
    
    # Performance optimization
    enable_caching: bool = True
    parallel_processing: bool = True
    max_workers: int = 4
    chunk_size: int = 10000
    
    # Categorical features
    categorical_encoding: str = "target"  # target, onehot, label, binary
    max_cardinality: int = 50
    enable_binning: bool = True
    binning_strategy: str = "quantile"  # quantile, uniform, kmeans


@dataclass
class FeatureMetadata:
    """–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö"""
    
    feature_names: List[str] = field(default_factory=list)
    feature_types: Dict[str, str] = field(default_factory=dict)
    feature_importance: Dict[str, float] = field(default_factory=dict)
    creation_time: Dict[str, float] = field(default_factory=dict)
    feature_stats: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    correlation_matrix: Optional[pd.DataFrame] = None
    selected_features: List[str] = field(default_factory=list)
    removed_features: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return {
            'feature_names': self.feature_names,
            'feature_types': self.feature_types,
            'feature_importance': self.feature_importance,
            'creation_time': self.creation_time,
            'feature_stats': self.feature_stats,
            'selected_features': self.selected_features,
            'removed_features': self.removed_features,
        }


class BaseFeatureGenerator(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - enterprise pattern"""
    
    @abstractmethod
    def generate_features(
        self, 
        data: pd.DataFrame, 
        target: Optional[pd.Series] = None
    ) -> pd.DataFrame:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        pass
    
    @abstractmethod
    def get_feature_names(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–º–µ–Ω —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        pass
    
    @abstractmethod
    def get_feature_types(self) -> Dict[str, str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        pass


class FeatureEngineer(BaseEstimator, TransformerMixin):
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π Feature Engineer –¥–ª—è XGBoost –º–æ–¥–µ–ª–µ–π
    
    –†–µ–∞–ª–∏–∑—É–µ—Ç enterprise patterns:
    - –ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å –ø–æ–¥–∫–ª—é—á–∞–µ–º—ã–º–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞–º–∏
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    - –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è tree –º–æ–¥–µ–ª–µ–π
    """
    
    def __init__(self, config: Optional[FeatureConfig] = None):
        self.config = config or FeatureConfig()
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞
        self.feature_generators_: List[BaseFeatureGenerator] = []
        self.scalers_: Dict[str, Any] = {}
        self.feature_selector_: Optional[Any] = None
        self.metadata_: FeatureMetadata = FeatureMetadata()
        self.is_fitted_: bool = False
        
        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        self._feature_cache: Dict[str, pd.DataFrame] = {}
        self._stats_cache: Dict[str, Dict[str, Any]] = {}
        
        self.console = Console()
        self._setup_logging()
        self._initialize_generators()
    
    def _setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        logger.add(
            f"logs/feature_engineer_{datetime.now():%Y%m%d}.log",
            rotation="daily",
            retention="30 days", 
            level="INFO"
        )
    
    def _initialize_generators(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        
        logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã
        from .technical_indicators import TechnicalIndicators
        from .statistical_features import StatisticalFeatures
        from .lagging_features import LaggingFeatures
        from .interaction_features import InteractionFeatures
        from .volatility_features import VolatilityFeatures
        
        # –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if self.config.enable_technical_indicators:
            self.feature_generators_.append(TechnicalIndicators(self.config))
        
        if self.config.enable_statistical_features:
            self.feature_generators_.append(StatisticalFeatures(self.config))
        
        if self.config.enable_lag_features:
            self.feature_generators_.append(LaggingFeatures(self.config))
        
        if self.config.enable_interaction_features:
            self.feature_generators_.append(InteractionFeatures(self.config))
        
        if self.config.enable_volatility_features:
            self.feature_generators_.append(VolatilityFeatures(self.config))
        
        logger.info(f"‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(self.feature_generators_)} –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤")
    
    def _validate_input_data(self, X: pd.DataFrame) -> pd.DataFrame:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        logger.info("üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        X = X.copy()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ
        if X.empty:
            raise ValueError("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        if X.columns.duplicated().any():
            logger.warning("‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏, —É–¥–∞–ª—è–µ–º...")
            X = X.loc[:, ~X.columns.duplicated()]
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        missing_ratio = X.isnull().sum() / len(X)
        high_missing_cols = missing_ratio[missing_ratio > 0.5].index.tolist()
        
        if high_missing_cols:
            logger.warning(f"‚ö†Ô∏è –£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å >50% –ø—Ä–æ–ø—É—Å–∫–æ–≤: {high_missing_cols}")
            X = X.drop(columns=high_missing_cols)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –ø—Ä–æ–ø—É—Å–∫–æ–≤
        if self.config.handle_missing == "forward_fill":
            X = X.fillna(method='ffill').fillna(method='bfill')
        elif self.config.handle_missing == "interpolate":
            X = X.interpolate().fillna(method='bfill')
        elif self.config.handle_missing == "drop":
            X = X.dropna()
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ inf –∑–Ω–∞—á–µ–Ω–∏–π
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(X.median())
        
        logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω—ã: {X.shape}")
        
        return X
    
    def _detect_feature_types(self, X: pd.DataFrame) -> Dict[str, str]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        
        feature_types = {}
        
        for col in X.columns:
            dtype = X[col].dtype
            unique_values = X[col].nunique()
            unique_ratio = unique_values / len(X)
            
            if dtype in ['object', 'category']:
                if unique_values <= self.config.max_cardinality:
                    feature_types[col] = FeatureType.CATEGORICAL.value
                else:
                    feature_types[col] = FeatureType.CATEGORICAL.value  # Large cardinality
            elif dtype == 'bool':
                feature_types[col] = FeatureType.BINARY.value
            elif unique_values == 2:
                feature_types[col] = FeatureType.BINARY.value
            elif unique_values < 10 and unique_ratio < 0.1:
                feature_types[col] = FeatureType.ORDINAL.value
            else:
                feature_types[col] = FeatureType.NUMERICAL.value
        
        return feature_types
    
    def _remove_outliers(self, X: pd.DataFrame) -> pd.DataFrame:
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤"""
        
        if not self.config.remove_outliers:
            return X
        
        logger.info(f"üßπ –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –º–µ—Ç–æ–¥–æ–º {self.config.outlier_method}...")
        
        X_clean = X.copy()
        numerical_cols = X.select_dtypes(include=[np.number]).columns
        
        if self.config.outlier_method == "iqr":
            for col in numerical_cols:
                Q1 = X[col].quantile(0.25)
                Q3 = X[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                # Clipping instead of removal to preserve data
                X_clean[col] = X[col].clip(lower_bound, upper_bound)
        
        elif self.config.outlier_method == "zscore":
            for col in numerical_cols:
                z_scores = np.abs(stats.zscore(X[col]))
                # Clip values beyond 3 standard deviations
                threshold_indices = z_scores > 3
                if threshold_indices.any():
                    median_val = X[col].median()
                    X_clean.loc[threshold_indices, col] = median_val
        
        logger.info("‚úÖ –í—ã–±—Ä–æ—Å—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
        
        return X_clean
    
    def _create_base_features(
        self, 
        X: pd.DataFrame, 
        y: Optional[pd.Series] = None
    ) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤—Å–µ–º–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞–º–∏"""
        
        logger.info("üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        all_features = [X]  # –ù–∞—á–∏–Ω–∞–µ–º —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        if self.config.parallel_processing:
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
                
                future_to_generator = {
                    executor.submit(generator.generate_features, X, y): generator
                    for generator in self.feature_generators_
                }
                
                with Progress(
                    SpinnerColumn(),
                    TextColumn("[progress.description]{task.description}"),
                    BarColumn(),
                    TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                ) as progress:
                    
                    task = progress.add_task("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...", total=len(self.feature_generators_))
                    
                    for future in as_completed(future_to_generator):
                        generator = future_to_generator[future]
                        
                        try:
                            features = future.result()
                            if features is not None and not features.empty:
                                all_features.append(features)
                                
                                # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                                gen_feature_names = generator.get_feature_names()
                                gen_feature_types = generator.get_feature_types()
                                
                                self.metadata_.feature_names.extend(gen_feature_names)
                                self.metadata_.feature_types.update(gen_feature_types)
                                
                            logger.info(f"‚úÖ {generator.__class__.__name__}: {len(features.columns) if features is not None else 0} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
                            
                        except Exception as e:
                            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ {generator.__class__.__name__}: {e}")
                        
                        progress.advance(task)
        else:
            # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
            for generator in self.feature_generators_:
                try:
                    features = generator.generate_features(X, y)
                    if features is not None and not features.empty:
                        all_features.append(features)
                        
                        gen_feature_names = generator.get_feature_names()
                        gen_feature_types = generator.get_feature_types()
                        
                        self.metadata_.feature_names.extend(gen_feature_names)
                        self.metadata_.feature_types.update(gen_feature_types)
                        
                    logger.info(f"‚úÖ {generator.__class__.__name__}: {len(features.columns) if features is not None else 0} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ {generator.__class__.__name__}: {e}")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if len(all_features) > 1:
            X_features = pd.concat(all_features, axis=1)
            
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            X_features = X_features.loc[:, ~X_features.columns.duplicated()]
        else:
            X_features = X
        
        logger.info(f"üéØ –°–æ–∑–¥–∞–Ω–æ {len(X_features.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–±—ã–ª–æ {len(X.columns)})")
        
        return X_features
    
    def _apply_scaling(self, X: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        
        if self.config.scaling_method == ScalingMethod.NONE:
            return X
        
        logger.info(f"üìè –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è: {self.config.scaling_method.value}")
        
        X_scaled = X.copy()
        numerical_cols = X.select_dtypes(include=[np.number]).columns
        
        for col in numerical_cols:
            if col not in self.scalers_:
                if self.config.scaling_method == ScalingMethod.STANDARD:
                    scaler = StandardScaler()
                elif self.config.scaling_method == ScalingMethod.ROBUST:
                    scaler = RobustScaler()
                elif self.config.scaling_method == ScalingMethod.MINMAX:
                    scaler = MinMaxScaler()
                elif self.config.scaling_method == ScalingMethod.QUANTILE:
                    scaler = QuantileTransformer()
                elif self.config.scaling_method == ScalingMethod.LOG:
                    # Log transform (add 1 to handle zeros)
                    X_scaled[col] = np.log1p(X[col] - X[col].min() + 1)
                    continue
                elif self.config.scaling_method == ScalingMethod.SQRT:
                    # Square root transform
                    X_scaled[col] = np.sqrt(X[col] - X[col].min() + 1)
                    continue
                
                self.scalers_[col] = scaler
                X_scaled[col] = scaler.fit_transform(X[[col]]).flatten()
            else:
                X_scaled[col] = self.scalers_[col].transform(X[[col]]).flatten()
        
        return X_scaled
    
    def _apply_binning(self, X: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ binning –¥–ª—è categorical features"""
        
        if not self.config.enable_binning:
            return X
        
        logger.info("üóÇÔ∏è –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ binning –¥–ª—è categorical features...")
        
        X_binned = X.copy()
        numerical_cols = X.select_dtypes(include=[np.number]).columns
        
        for col in numerical_cols:
            # –°–æ–∑–¥–∞–µ–º binned –≤–µ—Ä—Å–∏–∏ numerical features
            try:
                if self.config.binning_strategy == "quantile":
                    X_binned[f'{col}_binned'] = pd.qcut(
                        X[col], q=10, labels=False, duplicates='drop'
                    )
                elif self.config.binning_strategy == "uniform":
                    X_binned[f'{col}_binned'] = pd.cut(
                        X[col], bins=10, labels=False, duplicates='drop'
                    )
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å binning –∫ {col}: {e}")
        
        return X_binned
    
    def _select_features(
        self, 
        X: pd.DataFrame, 
        y: Optional[pd.Series] = None
    ) -> pd.DataFrame:
        """–û—Ç–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        
        if not self.config.enable_feature_selection or y is None:
            return X
        
        logger.info(f"üéØ –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ—Ç–æ–¥–æ–º {self.config.feature_selection_method}...")
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–∏–∑–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π
        variance_selector = VarianceThreshold(threshold=self.config.variance_threshold)
        X_variance_selected = pd.DataFrame(
            variance_selector.fit_transform(X),
            columns=X.columns[variance_selector.get_support()],
            index=X.index
        )
        
        removed_by_variance = set(X.columns) - set(X_variance_selected.columns)
        if removed_by_variance:
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ –ø–æ variance threshold: {len(removed_by_variance)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            self.metadata_.removed_features.extend(list(removed_by_variance))
        
        # –û—Å–Ω–æ–≤–Ω–æ–π feature selection
        if self.config.feature_selection_method == "mutual_info":
            if y.dtype == 'object' or y.nunique() < 10:
                selector = SelectKBest(
                    score_func=mutual_info_classif,
                    k=min(self.config.feature_selection_k, len(X_variance_selected.columns))
                )
            else:
                selector = SelectKBest(
                    score_func=mutual_info_regression,
                    k=min(self.config.feature_selection_k, len(X_variance_selected.columns))
                )
        elif self.config.feature_selection_method == "f_score":
            if y.dtype == 'object' or y.nunique() < 10:
                selector = SelectKBest(
                    score_func=f_classif,
                    k=min(self.config.feature_selection_k, len(X_variance_selected.columns))
                )
            else:
                selector = SelectKBest(
                    score_func=f_regression,
                    k=min(self.config.feature_selection_k, len(X_variance_selected.columns))
                )
        else:
            return X_variance_selected
        
        self.feature_selector_ = selector
        X_selected = pd.DataFrame(
            selector.fit_transform(X_variance_selected, y),
            columns=X_variance_selected.columns[selector.get_support()],
            index=X.index
        )
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        selected_features = list(X_selected.columns)
        removed_features = set(X_variance_selected.columns) - set(selected_features)
        
        self.metadata_.selected_features = selected_features
        self.metadata_.removed_features.extend(list(removed_features))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if hasattr(selector, 'scores_'):
            for i, feature in enumerate(X_variance_selected.columns):
                if selector.get_support()[i]:
                    self.metadata_.feature_importance[feature] = selector.scores_[i]
        
        logger.info(f"‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ {len(selected_features)} –∏–∑ {len(X.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        return X_selected
    
    def _calculate_feature_stats(self, X: pd.DataFrame):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º"""
        
        logger.info("üìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        for col in X.columns:
            stats_dict = {
                'dtype': str(X[col].dtype),
                'missing_ratio': X[col].isnull().sum() / len(X),
                'unique_values': X[col].nunique(),
                'unique_ratio': X[col].nunique() / len(X),
            }
            
            if X[col].dtype in ['int64', 'float64']:
                stats_dict.update({
                    'mean': X[col].mean(),
                    'std': X[col].std(),
                    'min': X[col].min(),
                    'max': X[col].max(),
                    'q25': X[col].quantile(0.25),
                    'median': X[col].median(),
                    'q75': X[col].quantile(0.75),
                    'skewness': X[col].skew(),
                    'kurtosis': X[col].kurtosis(),
                })
            
            self.metadata_.feature_stats[col] = stats_dict
    
    def fit(
        self, 
        X: Union[pd.DataFrame, np.ndarray], 
        y: Optional[Union[pd.Series, np.ndarray]] = None
    ) -> 'FeatureEngineer':
        """
        –û–±—É—á–µ–Ω–∏–µ feature engineer
        
        Args:
            X: –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            y: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        
        start_time = time.time()
        logger.info("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è Feature Engineer...")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ DataFrame –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if isinstance(X, np.ndarray):
            X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])
        
        if isinstance(y, np.ndarray):
            y = pd.Series(y, name='target')
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        X = self._validate_input_data(X)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        base_feature_types = self._detect_feature_types(X)
        self.metadata_.feature_types.update(base_feature_types)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤
        X = self._remove_outliers(X)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X_features = self._create_base_features(X, y)
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ binning
        X_features = self._apply_binning(X_features)
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        X_features = self._apply_scaling(X_features)
        
        # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X_final = self._select_features(X_features, y)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
        self._calculate_feature_stats(X_final)
        
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞
        if len(X_final.columns) <= 100:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            self.metadata_.correlation_matrix = X_final.corr()
        
        self.is_fitted_ = True
        
        training_time = time.time() - start_time
        logger.info(f"‚úÖ Feature Engineer –æ–±—É—á–µ–Ω –∑–∞ {training_time:.2f}—Å")
        logger.info(f"üìä –ò—Ç–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(X_final.columns)}")
        
        self._display_summary()
        
        return self
    
    def transform(self, X: Union[pd.DataFrame, np.ndarray]) -> pd.DataFrame:
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ feature engineering –∫ –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
        
        Args:
            X: –ù–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            
        Returns:
            –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        
        if not self.is_fitted_:
            raise ValueError("Feature Engineer –Ω–µ –æ–±—É—á–µ–Ω. –í—ã–∑–æ–≤–∏—Ç–µ fit() —Å–Ω–∞—á–∞–ª–∞.")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ DataFrame –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if isinstance(X, np.ndarray):
            X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        X = self._validate_input_data(X)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤
        X = self._remove_outliers(X)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–±–µ–∑ target)
        X_features = self._create_base_features(X, y=None)
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ binning
        X_features = self._apply_binning(X_features)
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        X_features = self._apply_scaling(X_features)
        
        # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø—Ä–∏–º–µ–Ω—è–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–π selector)
        if self.feature_selector_ is not None:
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏
            available_features = [col for col in self.metadata_.selected_features if col in X_features.columns]
            X_final = X_features[available_features]
        else:
            X_final = X_features
        
        return X_final
    
    def fit_transform(
        self, 
        X: Union[pd.DataFrame, np.ndarray], 
        y: Optional[Union[pd.Series, np.ndarray]] = None
    ) -> pd.DataFrame:
        """–û–±—É—á–µ–Ω–∏–µ –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∑–∞ –æ–¥–∏–Ω –≤—ã–∑–æ–≤"""
        
        self.fit(X, y)
        return self.transform(X)
    
    def _display_summary(self):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ summary —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        
        table = Table(title="üéØ FEATURE ENGINEERING SUMMARY")
        
        table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="cyan")
        table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", style="green")
        
        table.add_row("–í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", str(len(self.metadata_.feature_names)))
        table.add_row("–û—Ç–æ–±—Ä–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", str(len(self.metadata_.selected_features)))
        table.add_row("–£–¥–∞–ª–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", str(len(self.metadata_.removed_features)))
        table.add_row("–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∞–∫—Ç–∏–≤–Ω–æ", str(len(self.feature_generators_)))
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        type_counts = {}
        for feature_type in self.metadata_.feature_types.values():
            type_counts[feature_type] = type_counts.get(feature_type, 0) + 1
        
        for ftype, count in type_counts.items():
            table.add_row(f"–¢–∏–ø {ftype}", str(count))
        
        self.console.print(table)
    
    def get_feature_importance(self, top_k: Optional[int] = None) -> pd.DataFrame:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        
        if not self.metadata_.feature_importance:
            logger.warning("‚ö†Ô∏è –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ –≤—ã—á–∏—Å–ª–µ–Ω–∞")
            return pd.DataFrame()
        
        importance_df = pd.DataFrame([
            {'feature': feature, 'importance': importance}
            for feature, importance in self.metadata_.feature_importance.items()
        ]).sort_values('importance', ascending=False)
        
        if top_k:
            importance_df = importance_df.head(top_k)
        
        return importance_df
    
    def plot_feature_importance(
        self, 
        top_k: int = 20,
        save_path: Optional[str] = None
    ):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        
        importance_df = self.get_feature_importance(top_k)
        
        if importance_df.empty:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            return
        
        fig, ax = plt.subplots(figsize=(12, 8))
        
        bars = ax.barh(
            range(len(importance_df)), 
            importance_df['importance'],
            color='skyblue',
            alpha=0.7
        )
        
        ax.set_yticks(range(len(importance_df)))
        ax.set_yticklabels(importance_df['feature'])
        ax.set_xlabel('Feature Importance Score')
        ax.set_title(f'Top {top_k} Feature Importance')
        ax.grid(True, alpha=0.3)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π
        for i, bar in enumerate(bars):
            width = bar.get_width()
            ax.text(width, bar.get_y() + bar.get_height()/2,
                   f'{width:.3f}', ha='left', va='center')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
        else:
            plt.show()
    
    def plot_correlation_heatmap(
        self,
        features: Optional[List[str]] = None,
        save_path: Optional[str] = None
    ):
        """–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π"""
        
        if self.metadata_.correlation_matrix is None:
            logger.warning("‚ö†Ô∏è –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –Ω–µ –≤—ã—á–∏—Å–ª–µ–Ω–∞")
            return
        
        corr_matrix = self.metadata_.correlation_matrix
        
        if features:
            available_features = [f for f in features if f in corr_matrix.columns]
            corr_matrix = corr_matrix.loc[available_features, available_features]
        
        fig, ax = plt.subplots(figsize=(12, 10))
        
        sns.heatmap(
            corr_matrix,
            annot=False,
            cmap='coolwarm',
            center=0,
            square=True,
            ax=ax
        )
        
        ax.set_title('Feature Correlation Matrix')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"üìä –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_path}")
        else:
            plt.show()
    
    def save_engineer(self, path: Union[str, Path]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ feature engineer"""
        
        path = Path(path)
        path.mkdir(parents=True, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config_dict = {
            'enable_technical_indicators': self.config.enable_technical_indicators,
            'enable_statistical_features': self.config.enable_statistical_features,
            'enable_lag_features': self.config.enable_lag_features,
            'enable_interaction_features': self.config.enable_interaction_features,
            'enable_volatility_features': self.config.enable_volatility_features,
            'technical_periods': self.config.technical_periods,
            'statistical_windows': self.config.statistical_windows,
            'max_lags': self.config.max_lags,
            'scaling_method': self.config.scaling_method.value,
            # ... –¥–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        }
        
        with open(path / "config.json", 'w') as f:
            json.dump(config_dict, f, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        with open(path / "metadata.json", 'w') as f:
            json.dump(self.metadata_.to_dict(), f, indent=2, default=str)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫–µ–π–ª–µ—Ä–æ–≤
        if self.scalers_:
            joblib.dump(self.scalers_, path / "scalers.pkl")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ feature selector
        if self.feature_selector_:
            joblib.dump(self.feature_selector_, path / "feature_selector.pkl")
        
        logger.info(f"üíæ Feature Engineer —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {path}")
    
    @classmethod
    def load_engineer(cls, path: Union[str, Path]) -> 'FeatureEngineer':
        """–ó–∞–≥—Ä—É–∑–∫–∞ feature engineer"""
        
        path = Path(path)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        with open(path / "config.json", 'r') as f:
            config_dict = json.load(f)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config = FeatureConfig(**config_dict)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞
        engineer = cls(config)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        with open(path / "metadata.json", 'r') as f:
            metadata_dict = json.load(f)
        
        engineer.metadata_ = FeatureMetadata(**metadata_dict)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–∫–µ–π–ª–µ—Ä–æ–≤
        scalers_path = path / "scalers.pkl"
        if scalers_path.exists():
            engineer.scalers_ = joblib.load(scalers_path)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ feature selector
        selector_path = path / "feature_selector.pkl"
        if selector_path.exists():
            engineer.feature_selector_ = joblib.load(selector_path)
        
        engineer.is_fitted_ = True
        
        logger.info(f"üìÇ Feature Engineer –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {path}")
        
        return engineer


# Utility functions –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
@jit(nopython=True, parallel=True)
def fast_rolling_mean(data: np.ndarray, window: int) -> np.ndarray:
    """–ë—ã—Å—Ç—Ä–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ"""
    
    n = len(data)
    result = np.empty(n)
    
    for i in prange(n):
        if i < window - 1:
            result[i] = np.nan
        else:
            result[i] = np.mean(data[i-window+1:i+1])
    
    return result


@jit(nopython=True)
def fast_percentage_change(data: np.ndarray) -> np.ndarray:
    """–ë—ã—Å—Ç—Ä–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è"""
    
    n = len(data)
    result = np.empty(n)
    result[0] = 0.0
    
    for i in range(1, n):
        if data[i-1] != 0:
            result[i] = (data[i] - data[i-1]) / data[i-1]
        else:
            result[i] = 0.0
    
    return result


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    logger.info("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Feature Engineer...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    n_samples = 1000
    dates = pd.date_range('2023-01-01', periods=n_samples, freq='1H')
    
    # –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ —Ü–µ–Ω–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤—ã
    price_data = pd.DataFrame(index=dates)
    price_data['open'] = 100 + np.cumsum(np.random.randn(n_samples) * 0.01)
    price_data['high'] = price_data['open'] + np.random.exponential(0.5, n_samples)
    price_data['low'] = price_data['open'] - np.random.exponential(0.5, n_samples)
    price_data['close'] = price_data['open'] + np.random.randn(n_samples) * 0.5
    price_data['volume'] = np.random.exponential(1000, n_samples)
    
    # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è - –±—É–¥—É—â–µ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã
    target = price_data['close'].pct_change(periods=5).shift(-5)  # 5-period forward return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ feature engineer
    config = FeatureConfig(
        enable_technical_indicators=True,
        enable_statistical_features=True,
        enable_lag_features=True,
        enable_interaction_features=False,  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è –±—ã—Å—Ç—Ä–æ—Ç—ã
        enable_volatility_features=True,
        feature_selection_k=50
    )
    
    engineer = FeatureEngineer(config)
    
    # –û–±—É—á–µ–Ω–∏–µ –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è
    X_transformed = engineer.fit_transform(price_data, target.dropna())
    
    logger.info(f"üìä –ò—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(price_data.columns)}")
    logger.info(f"üìä –°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(X_transformed.columns)}")
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    importance = engineer.get_feature_importance(top_k=10)
    if not importance.empty:
        logger.info(f"üéØ –¢–æ–ø-3 –ø—Ä–∏–∑–Ω–∞–∫–∞: {importance['feature'].iloc[:3].tolist()}")
    
    logger.info("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Feature Engineer –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")